{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# pytorch (very) fast tutorial"],"metadata":{"id":"SGBvfLAr2-pS"}},{"cell_type":"markdown","source":["***pytorch*** is a package designed to provide tools for fast development of Gradient-based deep learning (GBDL)\n","\n","The key data structure in pytorch is the multidimensional array, that is called a ***tensor***\n","\n","Lets play a little with tensors and their more frequent operations."],"metadata":{"id":"LIEoCC7TWttU"}},{"cell_type":"code","source":["import torch"],"metadata":{"id":"nFmG1oooYKUI","executionInfo":{"status":"ok","timestamp":1697725474266,"user_tz":-120,"elapsed":4575,"user":{"displayName":"Javier González Barreda","userId":"01452124118077352826"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"DtnUbSfBWlIM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697725480413,"user_tz":-120,"elapsed":398,"user":{"displayName":"Javier González Barreda","userId":"01452124118077352826"}},"outputId":"824a8355-5780-4bc8-faf3-7ecd725ac99b"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0.3561, -0.7537, -0.5583, -2.0276],\n","        [-0.8399,  0.3919, -1.6627, -0.9940],\n","        [ 0.4773,  0.3329, -0.5464, -1.1929]])\n"]}],"source":["a = torch.randn(3,4)\n","print(a)\n","#tensor([[-0.5871,  1.9131,  1.0906,  0.1642],\n","#        [ 0.6189,  2.2898, -1.1778, -0.2386],\n","#        [ 1.0007,  2.2037,  0.8421,  0.3906] ])"]},{"cell_type":"markdown","source":["Tensors use 0 based indexes. Every element in a tensor is a tensor:"],"metadata":{"id":"iseJPjJpWrwU"}},{"cell_type":"code","source":["a[1,1]"],"metadata":{"id":"uYMo6gsaZ5k8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697725487853,"user_tz":-120,"elapsed":432,"user":{"displayName":"Javier González Barreda","userId":"01452124118077352826"}},"outputId":"c3bb4b8e-bacb-4c59-bbf9-fc9848483437"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.3919)"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["If the value of a tensor is wanted, then ..."],"metadata":{"id":"qkXl5BYba8hQ"}},{"cell_type":"code","source":["a[1,1].item()"],"metadata":{"id":"TAW3kjO2a5BT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697725543306,"user_tz":-120,"elapsed":511,"user":{"displayName":"Javier González Barreda","userId":"01452124118077352826"}},"outputId":"d2a44ef8-78c1-43b8-d76f-736c26de3c78"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.39187049865722656"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["tensors can be accessed with slices"],"metadata":{"id":"UYhQohhJbd9N"}},{"cell_type":"code","source":["print(a[0,:])\n","a[:,0]"],"metadata":{"id":"RKpJAZPGbr-i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697725557482,"user_tz":-120,"elapsed":566,"user":{"displayName":"Javier González Barreda","userId":"01452124118077352826"}},"outputId":"62bf761d-3c07-43ea-ad17-be9966bd4769"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 0.3561, -0.7537, -0.5583, -2.0276])\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([ 0.3561, -0.8399,  0.4773])"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["Vectoriced operations can be run with tensors"],"metadata":{"id":"-Pf9HJ1UcEKh"}},{"cell_type":"code","source":["a = torch.randn(2,2)\n","print('a', a)\n","b = torch.randn(2,2)\n","print('b',b)\n","c = a + b # elementwise sum operation\n","print('a + b',c)\n","d = a * b # elementwise product operation\n","print('a * b',d)\n","e = a.mm(b) # matrix multiplication operation\n","print('A * B',e)\n","f = a.matmul(b[:,0]) # Matrix-vector multiplication\n","print('A * b',f)"],"metadata":{"id":"nF2Fv_LHcMls","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697725599127,"user_tz":-120,"elapsed":449,"user":{"displayName":"Javier González Barreda","userId":"01452124118077352826"}},"outputId":"a685e90d-132a-4277-b9b5-0d4bb0ded0a7"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["a tensor([[ 0.1216, -0.6360],\n","        [ 0.5570,  0.8360]])\n","b tensor([[0.4479, 0.7091],\n","        [0.1676, 0.2687]])\n","a + b tensor([[0.5695, 0.0731],\n","        [0.7245, 1.1047]])\n","a * b tensor([[ 0.0545, -0.4510],\n","        [ 0.0933,  0.2247]])\n","A * B tensor([[-0.0521, -0.0847],\n","        [ 0.3895,  0.6196]])\n","A * b tensor([-0.0521,  0.3895])\n"]}]},{"cell_type":"markdown","source":["A given tensor can be transformed to another tensor with different dimensions (we can arrenge its elements to other dimensions, e.g from 1 dimension to 2 or more dimensions)\n","Several examples are given bellow:"],"metadata":{"id":"h4SN-HdCfK3y"}},{"cell_type":"code","source":["a = torch.randn(2,2) # 2 x 2 dimensions\n","print(a)\n","b = a.unsqueeze(-1) # 2 x 2 x 1 dimensions\n","print(b)\n","b = a.unsqueeze(0) # 1 x 2 x 2 dimensions\n","print(b)\n","c = a.view(4,1) # rearrange data as 4 x 1\n","print(c)\n","a = torch.randn(2) # 1 x 2 dimensions\n","print(a)\n","b = a.unsqueeze(-1) # 1 x 1 x 1 dimensions\n","print(b)\n","c = a.expand(3,2)\n","print(c)"],"metadata":{"id":"tzMor2NofJ7f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697725662914,"user_tz":-120,"elapsed":522,"user":{"displayName":"Javier González Barreda","userId":"01452124118077352826"}},"outputId":"f7f27483-24f3-451a-acf6-08aac80efbdc"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.0457,  1.8301],\n","        [-0.0730,  0.7090]])\n","tensor([[[-0.0457],\n","         [ 1.8301]],\n","\n","        [[-0.0730],\n","         [ 0.7090]]])\n","tensor([[[-0.0457,  1.8301],\n","         [-0.0730,  0.7090]]])\n","tensor([[-0.0457],\n","        [ 1.8301],\n","        [-0.0730],\n","        [ 0.7090]])\n","tensor([-0.8775, -1.4500])\n","tensor([[-0.8775],\n","        [-1.4500]])\n","tensor([[-0.8775, -1.4500],\n","        [-0.8775, -1.4500],\n","        [-0.8775, -1.4500]])\n"]}]},{"cell_type":"markdown","source":["How can we detect if CUDA is available in pytorch?"],"metadata":{"id":"JIgc-D6rqOxF"}},{"cell_type":"code","source":["do_I_have_cuda = torch.cuda.is_available()\n","if do_I_have_cuda:\n","  device = torch.device('cuda')\n","  a = a.to(device)\n","else:\n","  device = torch.device('cpu')\n","  a = a.to(device)"],"metadata":{"id":"nmLtl606rc-s","executionInfo":{"status":"ok","timestamp":1697725768697,"user_tz":-120,"elapsed":619,"user":{"displayName":"Javier González Barreda","userId":"01452124118077352826"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["In ANN (Artificial Neural Networks) there is three basic ways to do backproagation when there are n examples for training it:\n","\n","*   ***Full Gradient Descent***: one step for all the n examples\n","*   ***Stochastic Gradient Descent***: one step for each example\n","*   ***Mini-batch Stochastic Gradient Descent***: n/m steps for all the n examples, m is the size of the mini-batch\n","\n","Why mini-batch is so commonly used? Because it provides a more stable gradient estimate and because of computational efficency."],"metadata":{"id":"pNxkgLs1sM5D"}},{"cell_type":"markdown","source":["***Autograd***: Automatic differentiation\n","\n","pytorch contains methods for automatically compute gradients (frequently used in the backpropagation phase during training)"],"metadata":{"id":"nOISEhXWuxjh"}},{"cell_type":"code","source":["x = torch.randn(1, requires_grad=True)\n","# x is a tnsor that will record gradients\n","print(x)\n","y = x.exp() # e^x\n","print(y)\n","y.backward() # For every [x1, ..., xk] values used to compute y\n","             #   dy/dx is computed and stored in x_i.grad\n","             # Here, dy/dx = e^x = y\n","print(x.grad, y)"],"metadata":{"id":"K9EAWDl6wHxR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The calls of backward accumulate, so if we do not want this, we can de-activated computing grads:"],"metadata":{"id":"RkDesG_oy53u"}},{"cell_type":"code","source":["x = torch.randn(1, requires_grad=True)\n","print(\"Original x:\", x)\n","\n","# Perform computations without accumulating gradients\n","with torch.no_grad():\n","    y = x.exp()  # e^x\n","    print(\"Computed y without gradient accumulation:\", y)\n","\n","# Enable gradient accumulation again\n","x.requires_grad_(True)\n","y = x.exp()\n","print(\"Computed y with gradient accumulation:\", y)\n","\n","# Backward pass to compute gradients\n","y.backward()\n","\n","# Gradients are now accumulated in x.grad\n","print(\"Gradients of x:\", x.grad)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e4-uHRoVW-Sq","executionInfo":{"status":"ok","timestamp":1697726071396,"user_tz":-120,"elapsed":610,"user":{"displayName":"Javier González Barreda","userId":"01452124118077352826"}},"outputId":"9bf3a6ef-d698-4159-a05f-54a6c40acb27"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Original x: tensor([-0.3780], requires_grad=True)\n","Computed y without gradient accumulation: tensor([0.6852])\n","Computed y with gradient accumulation: tensor([0.6852], grad_fn=<ExpBackward0>)\n","Gradients of x: tensor([0.6852])\n"]}]}]}